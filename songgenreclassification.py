# -*- coding: utf-8 -*-
"""SongGenreClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1moHsaSGWzwX7TRd2kdahfWGd6S2jfuCk

# SONG GENRE CLASSIFICATION project notebook

Pere Ginebra, Joan Domingo

APA Q1 2021/22

FIB UPC

# Readme

The dataset used in this project can be found here: https://www.kaggle.com/vicsuperman/prediction-of-music-genre

To run the whole notebook you can use the run all/restart and run all button. Some cells are used to look for the best parameters for a model and therefore take very long to execute, these are labeled by a text cell above them with a warning sign (**[ ! ]**), you can skip them if you just want to see the results.

# Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# Uncomment to upgrade packages
#!pip install pandas --upgrade --user --quiet
# !pip install numpy --upgrade --user --quiet
# !pip install scipy --upgrade --user --quiet
# !pip install statsmodels --upgrade --user --quiet
# !pip install scikit-learn --upgrade --user --quiet
# !pip install graphviz --upgrade --user --quiet
#!pip install dython  --upgrade --user --quiet
# %load_ext autoreload

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns
from dython.nominal import associations
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
#pd.set_option('precision', 3)

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn import preprocessing
from sklearn.impute import KNNImputer
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

from tensorflow import keras

import warnings
warnings.filterwarnings('ignore')

def confusion(true, pred, classes):
    """
    Function for pretty printing confusion matrices
    """
    cm =pd.DataFrame(confusion_matrix(true, pred), 
                     index=classes,
                     columns=classes)
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'
    return cm

"""Mount drive if the dataset is in your drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# 1. Preprocessing

### Feature selection
"""

#music_data = pd.read_csv("/content/drive/MyDrive/APA_Practica/music_genre.csv", header=0)
music_data = pd.read_csv("music_genre.csv", header=0)
# we're not going to use the artist or song name to predict the genre, and the other 2 are irrelevant to the genre of a song
music_data = music_data.drop(columns=['instance_id','obtained_date','artist_name','track_name'])

music_data.head()
music_data.describe()

"""### Missing Values"""

prediction_data = music_data
prediction_data = prediction_data.replace(-1, np.nan);
prediction_data = prediction_data.replace(' ', np.nan);
prediction_data = prediction_data.replace('?', np.nan);
print('Missing Values\n',prediction_data.isna().sum())

# There is a suspiciously similar number of missing values between all atributes (except duration_ms and tempo which have a bigger problem...)
find_na_rows = prediction_data.drop(columns=['duration_ms','tempo'])
find_na_rows[find_na_rows.isna().any(axis=1)]
# We confirm our suspicion, there are 5 rows full of missing values...
# If we remove all rows that only have "NaN" we see that we have fixed most of our problem.
prediction_data = prediction_data.dropna(how='all')
print(prediction_data.isna().sum())
# Now onto our duration_ms problem...
no_duration = prediction_data[prediction_data['duration_ms'].isna()]
no_duration['music_genre'].value_counts()

no_tempo = prediction_data[prediction_data['tempo'].isna()]
no_tempo['music_genre'].value_counts()
# We see that all genres are missing a similar number of missing duration values, 
# so we could remove them all and loose around 10% of each genre's entries and be left with around 4500 for each
# OR try to "guess" the missing duration values

"""We will treat missing values after treating the categorical variables and outliers

### Treat categorical attributes
"""

prediction_data['mode'].replace(['Minor','Major'], [0,1], inplace=True)
prediction_data = pd.get_dummies(data=prediction_data, columns=['key'])
#prediction_data['key'].replace(['A', 'A#', 'B', 'C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#'], [0,1,2,3,4,5,6,7,8,9,10,11], inplace=True)
#ho he provat per veure si la correlation canviava, pero no xd
prediction_data.tempo = prediction_data.tempo.astype('float64')
prediction_data.head()
prediction_data.describe()

"""### Outliers

Most attributes in our dataset are calculated and range from 0-1 or 0-100, so they generally do not need special outlier treatment. Tempo, loudness and duration are the ones that might need treatment.
- Tempo: ranges from 34 to 220 bpm which is a reasonable range for music.
- Loudness: ranges from -47 to +3 dB, has a mean of -9 and std of 6, so the lower values could be treated as outliers (or strange cases of very quiet music).
- Duration: ranges from 155s (2.6min) to 4497s (74.95 min) which also seems somewhat reasonable, but the upper threshold could also be treated as outliers.
"""

fig, axes = plt.subplots(1,2, gridspec_kw={'width_ratios': [2, 4]}, figsize=(10,5))
prediction_data.boxplot(column='tempo',ax=axes[0], whis=1.75)
prediction_data.hist(column='tempo',ax=axes[1])

"""We can see that, as predicted, tempo has a pretty good distribution and not many outlier candidates."""

fig, axes = plt.subplots(1,2, gridspec_kw={'width_ratios': [2, 4]}, figsize=(10,5))
prediction_data.boxplot(column='loudness',ax=axes[0], whis=1.75)
prediction_data.hist(column='loudness',ax=axes[1])

"""We can also see that loudness has some slightly extreme cases, but we considered they're still not outliers."""

prediction_data[prediction_data['loudness']<-42]

"""If we check the songs at this extreme we notice that these values still carry a great deal of information as most extremely quiet songs belong to the classical genre."""

fig, axes = plt.subplots(1,2, gridspec_kw={'width_ratios': [2, 4]}, figsize=(10,5))
prediction_data.boxplot(column='duration_ms',ax=axes[0], whis=1.75)
prediction_data.hist(column='duration_ms',ax=axes[1])

prediction_data[prediction_data['duration_ms']>1750000]
prediction_data.shape
prediction_data = prediction_data.drop(prediction_data[prediction_data['duration_ms']>1750000].index)
prediction_data.shape

"""When analyzing song durations is where we find the first clear cases of outliers. These are probably correct values, but they look to be special cases that do not represent their genre correctly and might affect our models, so we will drop them."""

fig, axes = plt.subplots(1,2, gridspec_kw={'width_ratios': [2, 4]}, figsize=(10,5))
prediction_data.boxplot(column='duration_ms',ax=axes[0], whis=1.75)
prediction_data.hist(column='duration_ms',ax=axes[1])

"""### Impute missing values"""

#impute by knn
prediction_aux = prediction_data.copy()
prediction_aux['music_genre'].replace(['Electronic', 'Anime', 'Jazz', 'Alternative', 'Country', 'Rap', 'Blues', 'Rock', 'Classical', 'Hip-Hop'], [0,1,2,3,4,5,6,7,8,9], inplace=True)

imputer = KNNImputer(n_neighbors=5)
prediction_aux = imputer.fit_transform(prediction_aux)

prediction_aux = pd.DataFrame(data = prediction_aux)
prediction_data['duration_ms'] = prediction_aux[3]
prediction_data['tempo'] = prediction_aux[10]
prediction_data[prediction_data.duration_ms.isna() & prediction_data.tempo.isna()]

prediction_data = prediction_data.dropna()
prediction_data[prediction_data.duration_ms.isna() & prediction_data.tempo.isna()]

"""### Train-Test split"""

X = prediction_data.loc[:,prediction_data.columns != 'music_genre']
y = prediction_data['music_genre']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""### Normalize variables

Since most of our variables are already normalized to the range [0,1] we will only have to manually normalize popularity, duration_ms, loudness and tempo. These will be normalized using a min-max scaler computed from the train-split data.
"""

rows_to_minmax = ['popularity', 'duration_ms', 'loudness', 'tempo']
min_max_scaler =  preprocessing.MinMaxScaler()
min_max_scaler.fit(X_train[rows_to_minmax]);
X_train[rows_to_minmax] = min_max_scaler.transform(X_train[rows_to_minmax])
X_test[rows_to_minmax] = min_max_scaler.transform(X_test[rows_to_minmax])
X_train.describe()
X_test.describe()

"""### Pre-processed data visualization

We will also separately normalize the full dataset with all samples  so we can visualize the whole initial set:
"""

rows_to_minmax = ['popularity', 'duration_ms', 'loudness', 'tempo']
min_max_scaler =  preprocessing.MinMaxScaler()
prediction_data[rows_to_minmax] = min_max_scaler.fit_transform(prediction_data[rows_to_minmax]);
prediction_data.describe()

"""First we will analize the distribution of each variable independently of genre"""

data_columns = ['popularity', 'acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'valence']
fig, ax = plt.subplots(figsize=(15,5))
prediction_data[data_columns].boxplot()
prediction_data[data_columns].hist(figsize=(20,10), layout=(2,6));

"""Now we will also look at the distribution of each variable according to each genre. Here we can start to notice some patterns and which variables might be more useful than others."""

prediction_data.boxplot(column='popularity', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='acousticness', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='danceability', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='duration_ms', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='energy', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='instrumentalness', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='liveness', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='loudness', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='speechiness', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='tempo', by='music_genre',figsize=(16,6));
prediction_data.boxplot(column='valence', by='music_genre',figsize=(16,6));
# we use music data for this because key hasn't been transformed yet:
music_data = music_data.sort_values('key')
music_data.hist(column='key',by='music_genre',figsize=(16,10))

"""**[ ! ]** This pltot takes a while to execute and display"""

sns.pairplot(prediction_data[data_columns+['music_genre']], hue='music_genre');

"""Finally we will take a look at the correlation between all our variables in a correlation matrix:"""

output = associations(prediction_data,nan_strategy='drop_samples',figsize=(18,18))

correlation = output['corr']
ax = output['ax']

correlation

"""# 2. Testing Classificators

## linear/quadratic methods

### LDA (Linear Discriminant Analysis)
"""

lda = LinearDiscriminantAnalysis().fit(X_train,y_train)
print('Priors: ',lda.priors_)
print('Means:')
means = pd.DataFrame(lda.means_)
means.columns=X_train.columns
means['genre'] = lda.classes_
means.loc[:,['genre']+X_train.columns.to_list()]
print('Coeficients:')
coefs = pd.DataFrame(lda.coef_)
coefs.columns = X_train.columns
coefs['genre'] = lda.classes_
coefs.loc[:,['genre']+X_train.columns.to_list()]
print('Intercepts:')
intercepts = pd.DataFrame(lda.intercept_)
intercepts['genre'] = lda.classes_
intercepts

y_pred = lda.predict(X_test)
print('LDA score: ', lda.score(X_test,y_test))
print('\nCross Validation Score: ',cross_val_score(lda,X_test,y_test,cv=5))
print('Confusion Matrix:')
confusion(y_test, y_pred, lda.classes_)

print(classification_report(y_test,y_pred))

test_trans = lda.transform(X_test)
fig, ax = plt.subplots(figsize=(10,10))
for genre in lda.classes_:
    plt.scatter(test_trans[:,0][y_test==genre], test_trans[:,1][y_test==genre], label=genre)
    plt.plot(test_trans[:,0][y_test==genre].mean(), test_trans[:,1][y_test==genre].mean(), 'k^', markersize=10)
ax.set_xlabel('LD1')
ax.set_ylabel('LD2')
plt.legend();

"""We can see that LDA certainly groups together sets of songs, but from their scores and how close they are we can tell that it doesn't do a great job of it. This being said, it does a slightly better job of telling appart classical music (and anime to a certain extent) from the rest. This is probably due to them being the most different genres of the bunch.

### QDA (Quadratic Discriminant Analysis)
"""

qda = QuadraticDiscriminantAnalysis().fit(X_train,y_train)
print('Priors:', qda.priors_)
print('Means:\n')
means = pd.DataFrame(qda.means_)
means.columns=X_train.columns
means['genre'] = qda.classes_
means.loc[:,['genre']+X_train.columns.to_list()]

y_pred = qda.predict(X_test)
print('QDA score: ', qda.score(X_test,y_test))
print('\nCross Validation Score: ',cross_val_score(qda,X_test,y_test,cv=5))
print('Confusion Matrix:')
confusion(y_test, y_pred, qda.classes_)

"""We can see that our score is even worse when using QDA, so using a fancier version of discriminant analysis will not work for our use case..."""

print(classification_report(y_test,y_pred))

"""###KNN (K Nearest Neighbors)

Plot the accuracy depending on the K value
"""

acc = []
from sklearn import metrics
for i in range(10,40):
    neigh = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)
    yhat = neigh.predict(X_test)
    acc.append(metrics.accuracy_score(y_test, yhat))

plt.figure(figsize=(10,6))
plt.plot(range(10,40),acc,color = 'blue',linestyle='dashed', 
         marker='o',markerfacecolor='red', markersize=10)
plt.title('accuracy vs. K Value')
plt.xlabel('K')
plt.ylabel('Accuracy')

"""**[ ! ]** Finding the best hyperparameter combination (K and distance metrics). Only execute to retest, as it takes a while:"""

ks = [35,36,37,38,39]
metrics = ['euclidean', 'manhattan', 'mahalanobis', 'minkowski']

model_knn = KNeighborsClassifier()

knn = GridSearchCV(estimator=model_knn,
                   param_grid={'n_neighbors':ks,
                               'metric':metrics
                   },
                   scoring=['accuracy', 'recall_macro'],
                   cv=5,
                   return_train_score=True,
                  refit='recall_macro')

model_4CV = knn.fit(X_train, y_train)
model_4CV.best_score_

model_4CV.best_params_

"""Training a KNN classifier with the best parameters found (manhatan distance calculation and 14 nearest neighbors). Execute this if you only want the final KNN results:"""

myknn = KNeighborsClassifier(n_neighbors=39, metric='manhattan')
myknn.fit(X_train, y_train);

y_pred = myknn.predict(X_test)

print('\nCross Validation Score: ',cross_val_score(myknn, X_test, y_test, cv=5))
print('\n',classification_report(y_test,y_pred))
confusion(y_test,y_pred, myknn.classes_)

"""### SVM (Support Vector Machine)

**[ ! ]** Finding the best C value for our SVM. Only execute to restest, as it takes a while:
"""

svm = SVC(class_weight='balanced', kernel = 'linear')
Cs = [0.1,1, 10, 100]

trc = GridSearchCV(estimator=svm,
                   param_grid={
                       'C': Cs,
                   },
                   scoring=['accuracy', 'recall_macro', 'f1_macro'],
                   cv=5,
                   return_train_score=True,
                   refit='f1_macro')

model_3CV = trc.fit(X_train, y_train)

model_3CV.best_score_

model_3CV.best_params_

"""Using a linear suport vector classificator we obtain a slightly better score than using LDA (0.53). With the exploration of the regularization parameter C, the best result is when C = 100, so we will train the best model obtained with this parameter. Execute this if you want the final SVM results:"""

svm = SVC(kernel='linear',class_weight='balanced', C=100)
svm.fit(X_train,y_train)

scores = cross_val_score(svm, X_test, y_test, cv=5)
scores_recall = cross_val_score(svm, X_test, y_test, cv=5,scoring='recall_macro')
scores_f_score = cross_val_score(svm, X_test, y_test, cv=5,scoring='f1_macro')

print('Accuracy: ')
np.mean(scores)

print('Recall (mean): ')
np.mean(scores_recall)

print('F-score (mean): ')
np.mean(scores_f_score)

y_pred = svm.predict(X_test)

print(classification_report(y_test,y_pred))

print('Confusion Matrix:')
confusion(y_test, y_pred, svm.classes_)

"""##non-linear methods

###MLP (Multilayer Perceptron)
"""

y_train_num = y_train.replace(['Alternative', 'Anime', 'Blues', 'Classical', 'Country', 'Electronic', 'Hip-Hop', 'Jazz', 'Rap', 'Rock'], [0,1,2,3,4,5,6,7,8,9])
y_test_num = y_test.replace(['Alternative', 'Anime', 'Blues', 'Classical', 'Country', 'Electronic', 'Hip-Hop', 'Jazz', 'Rap', 'Rock'], [0,1,2,3,4,5,6,7,8,9])
earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True)

"""**[ ! ]** Finding the best architecture. Only execute this if you want to retest the results, as it may take a while."""

act = 'relu'
# first small architectures
#architectures = [(10,5), (12,9), (8,8), (6,12), (4,11)]
# these ones give us around 0.57-0.58 max accuracy:
architectures = [(96, 32, 64, 32, 22), (192, 64, 96, 24, 32, 64, 16, 32), (192, 64, 96, 24, 32, 64, 13), (200, 75, 100, 25, 40, 50, 15), (196,96,64,32,64,24,15), (500,150,100,125,50,75,200,125,150,50,25,15),(200,150,145,125,100,90,75,50,45,25,20,15,13)]
error = []
earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=7, restore_best_weights=True)
for layers in architectures:
    input = keras.Input(shape=(24,))
    model = keras.layers.Dense(layers[0], activation=act)(input)
    for layer in layers[1:]:
        model = keras.layers.Dense(layer, activation=act)(model)
    model = keras.layers.Dense(10, activation='softmax')(model)
    nn = keras.Model(input, model)

    nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    error.append(nn.fit(X_train, y_train_num, epochs=100, verbose=False, validation_data=(X_test,y_test_num), callbacks=[earlystopping]))

print('( first_layer_neurons, second_layer_neurons, ... )')
i = 0
for arch in architectures:
  if(i < len(error)):
    print(arch,':', error[i].history["accuracy"])
  i += 1

"""we will now use one of the best architectures we found, (200, 75, 100, 25, 40, 50, 15). Execute this if you only want the final MLP results"""

layers = (200, 75, 100, 25, 40, 50, 15)

input = keras.Input(shape=(24,))
model = keras.layers.Dense(layers[0], activation='relu')(input)
for layer in layers[1:]:
    model = keras.layers.Dense(layer, activation='relu')(model)
model = keras.layers.Dense(10, activation='softmax')(model)
nn = keras.Model(input, model)

nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
fitted_mlp = nn.fit(X_train, y_train_num, epochs=30, verbose=False, validation_data=(X_test,y_test_num), callbacks=[earlystopping])
print(fitted_mlp.history)

y_pred_aux = nn.predict(X_train)

y_pred = []
for line in y_pred_aux:
    max = 0
    max_pos = 0
    for i, score in enumerate(line):
        if score > max:
            max = score 
            max_pos = i
    y_pred.append(max_pos)

print(classification_report(y_train_num, y_pred, target_names=['Alternative', 'Anime', 'Blues', 'Classical', 'Country', 'Electronic', 'Hip-Hop', 'Jazz', 'Rap', 'Rock']))
confusion(y_train_num, y_pred, ['Alternative', 'Anime', 'Blues', 'Classical', 'Country', 'Electronic', 'Hip-Hop', 'Jazz', 'Rap', 'Rock'])

"""###Random Forest

**[ ! ]** Finding best random forest hyperparameters. Only execute to restest, as it takes a while:
"""

from sklearn.model_selection import RandomizedSearchCV


n_estimators = [100, 150, 200, 500]
max_depth = [5, 10, 15, 30, 50, None]
min_samples_split = [1, 2, 3]
min_samples_leaf = [1, 2, 3]
crit = ['gini']
random_grid = {'n_estimators': n_estimators,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'criterion': crit}

rf_model = RandomForestClassifier(class_weight='balanced')

trc = RandomizedSearchCV(estimator = rf_model, 
                         param_distributions = random_grid, 
                         n_iter = 100, 
                         cv = 5, 
                         verbose=2, 
                         random_state=42, 
                         n_jobs = -1,
                         scoring=['accuracy', 'f1', 'f1_macro','recall_macro'],
                         refit='recall_macro')

model_10CV = trc.fit(X_train, y_train)


model_10CV.best_score_

model_10CV.best_params_

"""Training a Random Forest classifier with the best parameters found. Execute this if you only want the final random forest results."""

rf = RandomForestClassifier(class_weight='balanced',
                                  max_depth=15,
                                  min_samples_leaf=2,
                                  min_samples_split=2,
                                  n_estimators=100);
rf.fit(X_train, y_train);

y_pred = rf.predict(X_test)
print('\nCross Validation Score: ',cross_val_score(rf,X_test,y_test))

print(classification_report(y_test, y_pred, target_names=rf.classes_))
confusion(y_test,y_pred, rf.classes_)